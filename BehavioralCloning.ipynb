{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D, Conv2D, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# file system settings\n",
    "IMG_PATH = 'track1/IMG/'\n",
    "IMAGE_METADATA_CSV = 'track1/driving_log.csv'\n",
    "TRACK2_METADATA_CSV = 'track2/driving_log.csv'\n",
    "FOLDER_SEPARATOR = '\\\\'\n",
    "\n",
    "# network parameters\n",
    "TURNING_OFFSET = 0.25\n",
    "TRAIN_VALID_SPLIT = 0.2\n",
    "TRAIN_EPOCHS = 3\n",
    "LEARN_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# learning settings\n",
    "FLIP_IMAGES = True\n",
    "USE_GAMMA_CORRECTION = True\n",
    "USE_TRACK2 = False\n",
    "USE_GENERATOR = False\n",
    "\n",
    "# debug settings\n",
    "DEBUG = True\n",
    "LIMIT_IMAGES_FOR_DEBUGGING = 128\n",
    "\n",
    "# settings for logging\n",
    "LOGFILE_NAME = 'logfile.txt'\n",
    "csv_logger = CSVLogger('log.csv', append=False, separator=';')\n",
    "\n",
    "\n",
    "# save whole stdout to file\n",
    "# sys.stdout = open(LOGFILE_NAME, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# method to import and measurements from csv\n",
    "def csv_to_array(filename):\n",
    "    lines = []\n",
    "    with open(filename) as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        for line in reader:\n",
    "            if DEBUG and (len(lines) == LIMIT_IMAGES_FOR_DEBUGGING):\n",
    "                break\n",
    "            lines.append(line)\n",
    "    return lines\n",
    "\n",
    "\n",
    "# method for image augmentation\n",
    "def image_augmentation(images_as_array):\n",
    "    images = []\n",
    "    measurements = []\n",
    "    for line in images_as_array:\n",
    "        for i in range(3):\n",
    "            source_path = line[i]\n",
    "            filename = source_path.split(FOLDER_SEPARATOR)[-1]\n",
    "            current_path = IMG_PATH + filename\n",
    "            image = cv2.imread(current_path)\n",
    "\n",
    "            if USE_GENERATOR:\n",
    "                # crop image here\n",
    "                image = image[60:140, 0:320]\n",
    "\n",
    "            images.append(image)\n",
    "            measurement = float(line[3])\n",
    "            # center image\n",
    "            if i == 0:\n",
    "                measurements.append(measurement)\n",
    "            # left image\n",
    "            elif i == 1:\n",
    "                measurements.append(measurement + TURNING_OFFSET)\n",
    "            # right image\n",
    "            elif i == 2:\n",
    "                measurements.append(measurement - TURNING_OFFSET)\n",
    "    # create more test data by flipping the images and inverting the corresponding turn angles\n",
    "    augmented_images, augmented_measurements = [], []\n",
    "    for image, measurement in zip(images, measurements):\n",
    "        augmented_images.append(image)\n",
    "        augmented_measurements.append(measurement)\n",
    "        # only add a flipped image of a turning image\n",
    "        if (FLIP_IMAGES and abs(measurement) > 0.2):\n",
    "            augmented_images.append(cv2.flip(image, 1))\n",
    "            augmented_measurements.append(measurement * -1.0)\n",
    "        if USE_GAMMA_CORRECTION:\n",
    "            # darken images\n",
    "            for y in range(-5, 0, 2):\n",
    "                augmented_images.append(gamma_correction(image, y))\n",
    "                augmented_measurements.append(measurement)\n",
    "            # brigthen images\n",
    "            for y in range(1, 6, 2):\n",
    "                augmented_images.append(gamma_correction(image, y))\n",
    "                augmented_measurements.append(measurement)\n",
    "    return augmented_images, augmented_measurements\n",
    "\n",
    "\n",
    "# TODO: add gamma correction for shadow simulation\n",
    "def gamma_correction(img, gamma):\n",
    "    # brighten or darken image\n",
    "    inv_gamma = 1.0 / gamma\n",
    "\n",
    "    table = np.array(\n",
    "        [((i / 255.0) ** inv_gamma) * 255\n",
    "         for i in np.arange(0, 256)]\n",
    "    ).astype(\"uint8\")\n",
    "\n",
    "    # apply gamma correction using the lookup table\n",
    "    return cv2.LUT(img, table)\n",
    "\n",
    "\n",
    "# method for batch processing with generators\n",
    "# TODO: Fix generator for long duration\n",
    "def generator(samples, batch_size=BATCH_SIZE):\n",
    "    num_samples = len(samples)\n",
    "    # Loop forever so the generator never terminates\n",
    "    while 1:\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset + batch_size]\n",
    "            print(\"Next Batch run with: \", offset)\n",
    "\n",
    "            X_train_temp, y_train_temp = image_augmentation(batch_samples)\n",
    "\n",
    "            X_train_gen = np.array(X_train_temp)\n",
    "            y_train_gen = np.array(y_train_temp)\n",
    "            yield shuffle(X_train_gen, y_train_gen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mneue\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:70: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "\n",
    "track_data = csv_to_array(IMAGE_METADATA_CSV)\n",
    "if USE_TRACK2:\n",
    "    track2_data = csv_to_array(TRACK2_METADATA_CSV)\n",
    "    track_data.extend(track2_data)\n",
    "    track2_data = []\n",
    "\n",
    "if USE_GENERATOR:\n",
    "    # use generators\n",
    "    train_samples, validation_samples = train_test_split(track_data, test_size=TRAIN_VALID_SPLIT)\n",
    "    # compile and train the model using the generator function\n",
    "    train_generator = generator(train_samples, batch_size=BATCH_SIZE)\n",
    "    validation_generator = generator(validation_samples, batch_size=BATCH_SIZE)\n",
    "\n",
    "else:\n",
    "    # create arrays of images and measurements\n",
    "    X_train, y_train = image_augmentation(track_data)\n",
    "    track_data = []\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2345 samples, validate on 587 samples\n",
      "Epoch 1/3\n",
      "1056/2345 [============>.................] - ETA: 30s - loss: 0.0458"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9f0121cc18d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m                                \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTRAIN_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                                callbacks=[csv_logger])\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# print layer information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2670\u001b[0m                     \u001b[1;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[1;32m-> 2672\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2654\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2655\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \"\"\"\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "if USE_GENERATOR:\n",
    "    model.add(Lambda(lambda x: (x / 127.5) - 1.0, input_shape=(80, 320, 3)))\n",
    "    # , output_shape=(80, 320, 3)\n",
    "else:\n",
    "    model.add(Cropping2D(cropping=((60, 20), (0, 0))))\n",
    "    model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(80, 320, 3)))\n",
    "    # crop 60 pixels from top, 20 from bottom, 0 from left and right\n",
    "\n",
    "model.add(Conv2D(24, (5, 5), strides=(2, 2), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(Conv2D(36, (5, 5), strides=(2, 2), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(Conv2D(48, (5, 5), strides=(2, 2), padding=\"valid\", activation=\"relu\"))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "# model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(1000))\n",
    "# model.add(Activation(\"relu\"))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# use mean squared error function with adam-optimizer\n",
    "model.compile(loss='mse', optimizer=Adam(LEARN_RATE))\n",
    "\n",
    "if USE_GENERATOR:\n",
    "    # use generator to train model\n",
    "    history_object = model.fit_generator(train_generator,\n",
    "                                         steps_per_epoch=len(train_samples),\n",
    "                                         validation_data=validation_generator,\n",
    "                                         validation_steps=len(validation_samples),\n",
    "                                         epochs=TRAIN_EPOCHS,\n",
    "                                         verbose=1,\n",
    "                                         callbacks=[csv_logger])\n",
    "else:\n",
    "    # train model\n",
    "    history_object = model.fit(X_train,\n",
    "                               y_train,\n",
    "                               validation_split=TRAIN_VALID_SPLIT,\n",
    "                               shuffle=True,\n",
    "                               epochs=TRAIN_EPOCHS,\n",
    "                               verbose=1,\n",
    "                               callbacks=[csv_logger])\n",
    "\n",
    "# print layer information\n",
    "with open(LOGFILE_NAME, 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        print(\"This run had the following settings:\")\n",
    "        print(\"Generator was used: \" + str(USE_GENERATOR))\n",
    "        print(\"Track2 was used: \" + str(USE_TRACK2))\n",
    "        print(\"Image augmentation was used with flipping images: \" + str(FLIP_IMAGES))\n",
    "        print(\"Number of epochs: \" + str(TRAIN_EPOCHS))\n",
    "        print(\"Learning rate: \" + str(LEARN_RATE))\n",
    "        print(\"Batch size: \" + str(BATCH_SIZE))\n",
    "\n",
    "        print(\"Keras layer summary: \")\n",
    "        model.summary()\n",
    "\n",
    "# save trained model to file\n",
    "model.save('model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training and validation loss as image\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.savefig('loss_visualization.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
